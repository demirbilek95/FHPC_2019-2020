Hyper Threading --> because of overlaping squares you won't get %100 speed.
It is like if you have 8 cores you can use it as almost 16 cores with hyperthreading
Generally bad for hpc because --> they are doing same thing with the different data. Overhead causing the disadvantage.

Opteron 6xxx AMD CPU
4 NUMA zone and 2 socket (every socket has two groups of cores and every group of cores has their own zones)

Intel uses QPI --> Quick PAth Interconntect


-------------------------------------------------------------------------------------------------------------------------

18.11.2019

Challenges or multicores (memory)
mpi processes can be changed (for cores) by linux os.
Tools --> Numactl, 
ccNuma --> cache coheriance ...
Stream benchmark (add ,scalar etc)
contention, compressure (slow donw calculation) -- reason for reducing stream results in ullyse
export OMP_ bla bla THREADS
BenchamrkingHPC-nodes


------------------------------------------------------------------------------------------------------

mpirun -np 4 ./mpi_pi.x 100000 2>/dev/null | grep walltime | sort -n -7

2>dev/null directs the output here

grep --> google it

sort -n (sort it numeric way) -k7 (please sort it seventh column for mpi_pi.x wall time)


GPU --> GRaphical Process Unit

GPGPU --> General Purporse Graphical Process Unit


grep
sort
wc
uniq

ulimit -a -- give you the limit

top --> check status of your machineirun 

In Ullyse

mpirun /bin/hostname | uniq

mpirun /bin/hostname | wc

mp --hostfile host /bin/hostname

	cat host --> files to be modified
		b13 slots = 50


convıncere
convencer

------------------------------------------------------------------------------------------------------

module load openmpi

echo $LD_LIBRARY_PATH

echo $PATH

mpi (tab +tab to complete the command)

mpicc <file>

mpif90 hello_world.F90 -o hello_world.x
-
ldd ./hello_world.x

./hello_world.x --> if I run command in this way, 

showbf --> show backfile windows

qsub -l nodes=1:ppn=4, walltime=00:30 -I

ldd ./hello_world.x

module load openmpi

./hello_world.x  --> cannot run

mpirun -np 1 ./hello_world.x

mpirun -np 1 ./hello_world.x --> asking 20 nodes, it fails because  it allows only 4 cores at the same time


----------------------------------------------------------------------------------


mpicc <file_name> -o mpi_env.x

mpirun ./mpi_env.x

uname -a --> to see the name of the machine

mpirun -np 1 ./send_message.x --> hata verir çünkü sadece bir node belirtiyoruz, nereye mesaj gönderip alacak?


---------------------------------------------------------------------------------------------

mpicc deadlock.c -deadlock.x

mpirun -np 2 ./deadlock.x










































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































