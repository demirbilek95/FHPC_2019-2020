{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strong Scaling Test\n",
    "**1-measure the time-to-solution of the two codes in a strong-scaling test (use some meaningful value for N, like $10^9$), using from 1 (using the serial version) to $N_c$ cores on a node;**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test the strong scaling of codes, $N = 10^9$ chosen and from each program was run using from 1 (serial versions) to 20 threads. Both wall-clock time and elapsed time considered and tested for each step.\n",
    "\n",
    "![Strong Scaling](1-exercise_0_strong_scaling(wall-clock_time).png)\n",
    "\n",
    "\n",
    "Above plots shows us, touch by all and touch by first scale similar up to 4 threads, after this point touch by first stopped scaling and touch by all continued to scale."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Speed Up (Wall-Clock) / # Threads | 1 | 2          | 4          | 8          | 12           | 16          | 20         | \n",
    "|-----------------------------------|---|------------|------------|------------|--------------|-------------|------------| \n",
    "| Touch By First                    | 1 | 1.97551078 | 3.83930992 | 6.76027407 |  7.71710233, |  7.95280571 | 8.09099644 | \n",
    "| Touch By All                      | 1 | 1.99257554 | 3.86824436 | 7.20652361 | 10.30201848  | 13.66230185 | 16.8955089 | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Strong Scaling](1-exercise_0_strong_scaling(elapsed_time).png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Elapsed Time / # Threads | 1    | 2    | 4    | 8    | 12   | 16   | 20   | \n",
    "|--------------------------|------|------|------|------|------|------|------| \n",
    "| Touch By First           | 6.19 | 4.79 | 4.11 | 3.81 | 3.77 | 3.77 | 3.78 | \n",
    "| Touch By All             | 6.19 | 3.15 | 1.62 | 0.88 | 0.62 | 0.52 | 0.48 | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Speed Up (Elapsed Time) / # Threads | 1 | 2          | 4          | 8          | 12         | 16          | 20          | \n",
    "|-------------------------------------|---|------------|------------|------------|------------|-------------|-------------| \n",
    "| Touch By First                      | 1 | 1.29227557 | 1.50608273 | 1.62467192 | 1.64190981 | 1.64190981  | 1.63756614  | \n",
    "| Touch By All                        | 1 | 1.96507937 | 3.82098765 | 7.03409091 | 9.98387097 | 11.90384615 | 12.89583333 | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above plots shows us touch by all scales better than touch by first in terms of elapsed times. Actually touch by first doesn't scale at all. On the other hand touch by all scales very good up to 16 threads but after this point, appropriately to the Amdahl's Law it stoped scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Overhead\n",
    "**2-measure the parallel overhead of both codes, from 2 to $N_c$ cores on a node;**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For measuring overhead two methods applied. First for both code serial fractions are calculated to understand if there is overhead or not. After that to estimate it one of the optimistic formula is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Serial Fraction](2-exercise_0_serial_fraction(wall-clock-time).png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Serial Fraction (Wall-clock Time) / # Threads | 2          | 4          | 8          | 12         | 16         | 20         | \n",
    "|-----------------------------------------------|------------|------------|------------|------------|------------|------------| \n",
    "| Touch By First                                | 0.0123964  | 0.0139513  | 0.02619771 | 0.05045344 | 0.06745791 | 0.07746755 | \n",
    "| Touch By All                                  | 0.00372606 | 0.01135361 | 0.01572933 | 0.01498366 | 0.01140705 | 0.00967087 | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Serial Fraction](2-exercise_0_serial_fraction(elapsed-time).png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Serial Fraction (Elapsed Time) / # Threads | 2          | 4          | 8          | 12         | 16         | 20         | \n",
    "|--------------------------------------------|------------|------------|------------|------------|------------|------------| \n",
    "| Touch By First                             | 0.54765751 | 0.55196554 | 0.56058158 | 0.57350565 | 0.58298331 | 0.5901709  | \n",
    "| Touch By All                               | 0.0177706  | 0.01561659 | 0.01961689 | 0.01835806 | 0.02294023 | 0.02899413 | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to serial fraction calculations change (increasing serial fraction means lack of scaling is also due to the parallelization overhead) on the other hand if it is stable lack of scaling is due to the serial workload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to measure estimated overhead for the codes, I have used general formula, overhead function $T_o = p \\times T_p-T_S$ [reference (page 2)](https://www8.cs.umu.se/kurser/5DV050/VT11/F1b.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Overhead](2-exercise_0_overhead(wall-clock-time).png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Overhead (Wall-clock Time) / # Threads | 1 | 2       | 4        | 8        | 12       | 16       | 20      | \n",
    "|----------------------------------------|---|---------|----------|----------|----------|----------|---------| \n",
    "| Touch By First                         | 0 | 0.03476 | 0.11736  | 0.514216 | 1.556208 | 2.83732  | 4.12722 | \n",
    "| Touch By All                           | 0 | 0.01045 | 0.095526 | 0.308798 | 0.46225  | 0.479878 | 0.51533 | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Serial Fraction](2-exercise_0_overhead(elapsed-time).png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Overhead Elapsed Time) / # Threads | 1 | 2    | 4     | 8     | 12    | 16    | 20    | \n",
    "|------------------------------------|---|------|-------|-------|-------|-------|-------| \n",
    "| Touch By First                     | 0 | 3.39 | 10.25 | 24.29 | 39.05 | 54.13 | 69.41 | \n",
    "| Touch By All                       | 0 | 0.11 | 0.29  | 0.85  | 1.25  | 2.13  | 3.41  | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of overhead with the increasing number of computation units touch by first method's overhead increase too much so this situation shows us touch by all method is more efficient than touch by first method. In order to understand this difference, deeper analyze must be performed. According to this context these codes will be profilied by using perf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling Codes\n",
    "**3-provide any relevant metrics that explain any observed difference;**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to specify difference between two codes, I have used perf to profile codes (collecting hardware, software events). There is no significant difference between two codes. In order to get statistically significant results data collection repeated 10 times. Results are from Ulysses (20 threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Perf Results](3-perf_stat.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is no significant difference between two codes. Chosen events and small differences will be explained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best case for cpu-cycle and instructions must be  multiple instructions are executed in a single cycle so in terms of instruction per cycle touch by all policy is better than touch by first but there is no big difference.\n",
    "- In terms of cache misses results for both code is similar even touch by first is bit better than touch by all. It seems that both codes don't perform well about cache so it cause execution delays by requiring the program to fetch the data from other cache level. (perf c2c will be used to analyze this event better)\n",
    "- Branch instructions and misses are more or less same for each codes and it looks efficient.\n",
    "- The cycles stalled in the front-end are waste because front-end doesn't feed the back-end but for these two code percentage is more or less same\n",
    "- Task clock shows time spent on the profiled task. So in this context we can say that touch by all policy is way better than touch by first since utilizaiton of CPU (with 20 threads) in other words parallelization of touch by all (usage of threads) are more efficient than touch by first.\n",
    "- To understand better, difference between two method perf c2c command used. In Ulyses perf c2c command can not be used so I used it in my local computer (with 8 threads). You can find my computers architecture and compiler info in readme file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Perf C2C](3-perf_c2c_results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to above image, touch by all (right) seems more efficient than touch by first (left) because the cache of each thread is warmed-up with the data. Cache hits are better for touch by all. There is no global shared cache line event for touch by first but touch by all has this event and naturally performs better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Part\n",
    "**figure out how you could allocate and correctly initialise the right amount of memory separately on each thread**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the optional part, I realized that opening two parallel region is inefficient and increase overhead so I combined them in one parallel region. Also I have used schedule directives and most suitable one is guided with chunk size 50. \n",
    "This change gave me smaller elapsed times but since I am initializing and summing the array in the same for loop wall-clock time increased. Also question is specifically about initializing array so I ignored the summing up without initializing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Strong Scalability with Touch By All"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perf Cache Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the difference between codes, I have used perf c2c record and report command in my computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Perf C2C](perf_c2c.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen, optional (right side) is more efficient global shared cache hits (except L2 cache hits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried to openmp-ize serial application of monte carlo pi. After few trials and modifications I got better run times than serial one. \n",
    "During development process generating random numbers were bit hard, first I applied standard routines but estimations of pi was terrible. That is why I changed the way and with the help of Appendix 1 and some google search (drand48_r function requires structure) I could obtain better code. Also opening regular parallel regions (without specifying private, shared) doesn't give better results than the serial one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Refer the code openmp_pi.c*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weak and Strong Scalability\n",
    "**1-establish its weak and strong scalability;**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strong Scalability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since elapsed time and walltime is more or less same, for this tests I'll only use elapsed time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![openmp-strong](1-1-strong_scalability_openmp.png)\n",
    "\n",
    "\n",
    "According to result of program there is super linear scaling up to 12 threads for $N=10^9$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Elapsed Time / # Threads |  | 1     | 2    | 4    | 8    | 12   | 16   | 20   | \n",
    "|--------------------------|--|-------|------|------|------|------|------|------| \n",
    "| OpenMP                   |  | 19.66 | 8.95 | 4.62 | 2.44 | 1.72 | 1.29 | 1.03 | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Speed Up (Elapsed Time) / # Threads |  | 1 | 2          | 4          | 8          | 12          | 16          | 20          | \n",
    "|-------------------------------------|--|---|------------|------------|------------|-------------|-------------|-------------| \n",
    "| OpenMP                              |  | 1 | 2.19664804 | 4.25541126 | 8.05737705 | 11.43023256 | 15.24031008 | 19.08737864 | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weak Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![openmp-weak](1-1-weak_scalability_openmp.png)\n",
    "\n",
    "According to logic of weak scalability when we increase the number of cores with the N run time\n",
    "supposed to be same. However for this example there are small differences. There might be some\n",
    "room for optimization of parallelization part. But after 12 threads run time became more or less constant which is good. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Elapsed Time (Weak Scaling) / # Threads |  | 2     | 4     | 8     | 12    | 16   | 20    | \n",
    "|-----------------------------------------|--|-------|-------|-------|-------|------|-------| \n",
    "| OpenMP                                  |  | 17.99 | 18.43 | 19.49 | 20.57 | 20.6 | 20.57 | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Overhead\n",
    "**2-estimate the parallel overhead;**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![serial_fraction](1-2-serial_fraciton_openmp.png)\n",
    "\n",
    "For serial fraction, above plot shows us first there is increase (which my be due to the overhead) after that it became mor or less constant so in order to estimate overhead $p \\times Tp - Ts$ will be used."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Serial Fraction (Elapsed Time) / # Threads |  | 2           | 4           | 8           | 12         | 16         | 20         | \n",
    "|--------------------------------------------|--|-------------|-------------|-------------|------------|------------|------------| \n",
    "| OpenMP                                     |  | -0.08952187 | -0.02000678 | -0.00101729 | 0.00453158 | 0.00332316 | 0.00251646 | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![overhead](1-2-overhead_openmp.png)\n",
    "\n",
    "Similar to strong scaling results, program scales perfectly up to 16 threads but after because of overhead it slowed down. It means that there might be still room for parallel optimizaiton especially for higher number of threads (16 and 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Overhead Elapsed Time) / # Threads | 1 | 2     | 4     | 8     | 12   | 16   | 20   | \n",
    "|------------------------------------|---|-------|-------|-------|------|------|------| \n",
    "| OpenMP                             | 0 | -1.76 | -1.18 | -0.14 | 0.98 | 0.98 | 0.94 | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing with OpenMPI\n",
    "**3-compare the performance of your OpenMP version and of the MPI version, in terms of time-to-solution and of parallel efficiency. Run the MPI version with $N_c$ processes (i.e. $N_c$ = the largest number of physical threads that you have on the node) both on the single node that you use for the OpenMP version and on multiple nodes (keeping constant the number of processes). That should allow you to understand the impact of the network and how good is the shared-memory implementation of the MPI library.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Node Comparision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![strong_scaling_openmp vs mpi](1-3-compare_strong_scaling.png)\n",
    "\n",
    "According to this comparasion, in single node openmp performs better than mpi paradigm. (scalability and run time is better than mpi paradigm.) MPI program stopped scaling after 8 cores but openmp scaled linearly up to 20 threads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Elapsed Time / # Threads |  | 1     | 2     | 4    | 8    | 12   | 16   | 20   | \n",
    "|--------------------------|--|-------|-------|------|------|------|------|------| \n",
    "| MPI                      |  | 21.5  | 11.79 | 6.69 | 4.33 | 3.6  | 3.18 | 2.95 | \n",
    "| OpenMP                   |  | 19.66 | 8.95  | 4.62 | 2.44 | 1.72 | 1.29 | 1.03 | \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Speed Up (Elapsed Time) / # Threads |  | 1 | 2          | 4          | 8          | 12          | 16          | 20          | \n",
    "|-------------------------------------|--|---|------------|------------|------------|-------------|-------------|-------------| \n",
    "| MPI                                 |  | 1 | 1.8235793  | 3.21375187 | 4.96535797 | 5.97222222  | 6.76100629  | 7.28813559  | \n",
    "| OpenMP                              |  | 1 | 2.19664804 | 4.25541126 | 8.05737705 | 11.43023256 | 15.24031008 | 19.08737864 | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Node Comparision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand difference between openmp and mpi paradigm same test will be done for multiple nodes with the same number of cores (I got 4 nodes and 5 cores for each node). In this way impact of network and shared memory implementation of mpi will be better understood. It doesn't make sense to run openmp program for more than one node but I run the openmp program for the same conditions to see what happens for openmp after that I will compare mpi program with single mpi performance, single node openmp performance verbally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![strong_scaling_openmp vs mpi](1-3-multiplenode_compare_strong_scaling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows us, in multiple nodes mpi paradigm works better. Openmp program didn't scale after 8 threads which is bigger than 5 because openmp is for shared memory. However in comparasion to mpi with single node and openmp single node, it doesn't perform bettern than them which shows communication between nodes makes program slower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Multiple Node Elapsed Time / # Threads |  | 1     | 2     | 4    | 8    | 12   | 16   | 20   | \n",
    "|----------------------------------------|--|-------|-------|------|------|------|------|------| \n",
    "| MPI                                    |  | 21.86 | 11.96 | 7.07 | 4.6  | 3.68 | 4.16 | 3.95 | \n",
    "| OpenMP                                 |  | 19.68 | 9.27  | 4.83 | 4.31 | 4.07 | 4.1  | 4.06 | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Multiple Node Speed Up / # Threads |  | 1 | 2          | 4          | 8          | 12         | 16         | 20         | \n",
    "|------------------------------------|--|---|------------|------------|------------|------------|------------|------------| \n",
    "| MPI                                |  | 1 | 1.8277592  | 3.09193777 | 4.75217391 | 5.94021739 | 5.25480769 | 5.53417722 | \n",
    "| OpenMP                             |  | 1 | 2.12297735 | 4.07453416 | 4.56612529 | 4.83538084 | 4.8        | 4.84729064 | \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
