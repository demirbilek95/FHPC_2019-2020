{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strong Scaling Test\n",
    "**1-measure the time-to-solution of the two codes in a strong-scaling test (use some meaningful value for N, like $10^9$), using from 1 (using the serial version) to $N_c$ cores on a node;**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test the strong scaling of codes, $N = 10^9$ chosen and from each program was run using from 1 (serial versions) to 20 threads. Both wall-clock time and elapsed time considered and tested for each step.\n",
    "\n",
    "![Strong Scaling](1-exercise_0_strong_scaling(wall-clock_time).png)\n",
    "\n",
    "- 01_array_sum_output.txt speed up: [1, 1.97551078, 3.83930992, 6.76027407, 7.71710233, 7.95280571, 8.09099644]\n",
    "- 06_touch_by_all_output.txt speed up: [ 1, 1.99257554, 3.86824436, 7.20652361, 10.30201848, 13.66230185,16.8955089]\n",
    "\n",
    "Above plots shows us, touch by all and touch by first scale similar up to 4 threads, after this point touch by first stopped scaling and touch by all continued to scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Strong Scaling](1-exercise_0_strong_scaling(elapsed_time).png)\n",
    "\n",
    "- 1 threads run has elapsed time:  6.19\n",
    "- 2 threads run has elapsed time:  4.79\n",
    "- 4 threads run has elapsed time:  4.11\n",
    "- 8 threads run has elapsed time:  3.81\n",
    "- 12 threads run has elapsed time:  3.77\n",
    "- 16 threads run has elapsed time:  3.77\n",
    "- 20 threads run has elapsed time:  3.78\n",
    "\n",
    "\n",
    "- 01_array_sum_output.txt speed up: [1, 1.29227557, 1.50608273, 1.62467192, 1.64190981, 1.64190981, 1.63756614]\n",
    "    \n",
    "\n",
    "- 1 threads run has elapsed time:  6.19\n",
    "- 2 threads run has elapsed time:  3.15\n",
    "- 4 threads run has elapsed time:  1.62\n",
    "- 8 threads run has elapsed time:  0.88\n",
    "- 12 threads run has elapsed time:  0.62\n",
    "- 16 threads run has elapsed time:  0.52\n",
    "- 20 threads run has elapsed time:  0.48\n",
    "\n",
    "\n",
    "- 06_touch_by_all_output.txt speed up: [ 1, 1.96507937, 3.82098765, 7.03409091, 9.98387097, 11.90384615, 12.89583333]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above plots shows us touch by all scales better than touch by first in terms of elapsed times. Actually touch by first doesn't scale at all. On the other hand touch by all scales very good up to 16 threads but after this point, appropriately to the Amdahl's Law it stoped scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Overhead\n",
    "**2-measure the parallel overhead of both codes, from 2 to $N_c$ cores on a node;**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For measuring overhead two methods applied. First for both code serial fractions are calculated to understand if there is overhead or not. After that to estimate it one of the optimistic formula is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Serial Fraction](2-exercise_0_serial_fraction(wall-clock-time).png)\n",
    "\n",
    "- Serial Fraction of 01_array_sum_output.txt : [0.0123964, 0.0139513 ,0.02619771 ,0.05045344, 0.06745791, 0.07746755]\n",
    "- Serial Fraction of 06_touch_by_all_output.txt : [0.00372606, 0.01135361, 0.01572933, 0.01498366, 0.01140705, 0.00967087]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Serial Fraction](2-exercise_0_serial_fraction(elapsed-time).png)\n",
    "\n",
    "- Serial Fraction of 01_array_sum_output.txt : [0.54765751, 0.55196554, 0.56058158, 0.57350565, 0.58298331, 0.5901709 ]\n",
    "- Serial Fraction of 06_touch_by_all_output.txt : [0.0177706, 0.01561659, 0.01961689, 0.01835806, 0.02294023, 0.02899413]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to serial fraction calculations change (increasing serial fraction means lack of scaling is also due to the parallelization overhead) on the other hand if it is stable lack of scaling is due to the serial workload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to measure estimated overhead for the codes, I have used general formula, overhead function $T_o = p \\times T_p-T_S$ [reference (page 2)](https://www8.cs.umu.se/kurser/5DV050/VT11/F1b.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Serial Fraction](2-exercise_0_overhead(wall-clock-time).png)\n",
    "\n",
    "- Overhead of 01_array_sum_output.txt : [0, 0.03476, 0.11736, 0.514216, 1.556208, 2.83732, 4.12722 ]\n",
    "- Overhead of 06_touch_by_all_output.txt : [0, 0.01045, 0.095526, 0.308798, 0.46225, 0.479878, 0.51533 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Serial Fraction](2-exercise_0_overhead(elapsed-time).png)\n",
    "\n",
    "- Overhead of 01_array_sum_output.txt : [ 0, 3.39, 10.25, 24.29, 39.05, 54.13, 69.41]\n",
    "- Overhead of 06_touch_by_all_output.txt : [0, 0.11, 0.29, 0.85, 1.25, 2.13, 3.41]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of overhead with the increasing number of computation units touch by first method's overhead increase too much so this situation shows us touch by all method is more efficient than touch by first method. In order to understand this difference, deeper analyze must be performed. According to this context these codes will be profilied by using perf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling Codes\n",
    "**3-provide any relevant metrics that explain any observed difference;**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to specify difference between two codes, I have used perf to profile codes (collecting hardware, software events). There is no significant difference between two codes. In order to get statistically significant results data collection repeated 10 times. Results are from Ulysses (20 threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Perf Results](3-perf_stat.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is no significant difference between two codes. Chosen events and small differences will be explained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best case for cpu-cycle and instructions must be  multiple instructions are executed in a single cycle so in terms of instruction per cycle touch by all policy is better than touch by first but there is no big difference.\n",
    "- In terms of cache misses results for both code is similar even touch by first is bit better than touch by all. It seems that both codes don't perform well about cache so it cause execution delays by requiring the program to fetch the data from other cache level. (perf c2c will be used to analyze this event better)\n",
    "- Branch instructions and misses are more or less same for each codes and it looks efficient.\n",
    "- The cycles stalled in the front-end are waste because front-end doesn't feed the back-end but for these two code percentage is more or less same\n",
    "- Task clock shows time spent on the profiled task. So in this context we can say that touch by all policy is way better than touch by first since utilizaiton of CPU (with 20 threads) in other words parallelization of touch by all (usage of threads) are more efficient than touch by first.\n",
    "- To understand better, difference between two method perf c2c command used. In Ulyses perf c2c command can not be used so I used it in my local computer (with 8 threads). You can find my computers architecture and compiler info in readme file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Perf C2C](3-perf_c2c_results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to above image, touch by all (right) seems more efficient than touch by first (left) because the cache of each thread is warmed-up with the data. Cache hits are better for touch by all. There is no global shared cache line event for touch by first but touch by all has this event and naturally performs better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried to openmp-ize serial application of monte carlo pi. After few trials and modifications I got better run times than serial one. \n",
    "During development process generating random numbers were bit hard, first I applied standard routines but estimations of pi was terrible. That is why I changed the way and with the help of Appendix 1 and some google search (drand48_r function requires structure) I could obtain better code. Also opening regular parallel regions (without specifying private, shared) doesn't give better results than the serial one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Refer the code openmp_pi.c*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weak and Strong Scalability\n",
    "**1-establish its weak and strong scalability;**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strong Scalability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since elapsed time and walltime is more or less same, for this tests I'll only use elapsed time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![openmp-strong](1-1-strong_scalability_openmp.png)\n",
    "\n",
    "- 1 threads run has elapsed time:  19.66\n",
    "- 2 threads run has elapsed time:  8.95\n",
    "- 4 threads run has elapsed time:  4.62\n",
    "- 8 threads run has elapsed time:  2.44\n",
    "- 12 threads run has elapsed time:  1.72\n",
    "- 16 threads run has elapsed time:  1.29\n",
    "- 20 threads run has elapsed time:  1.03\n",
    "\n",
    "Speed up: [1, 2.19664804, 4.25541126, 8.05737705, 11.43023256, 15.24031008, 19.08737864]\n",
    "\n",
    "According to result of program scales linearly for $N=10^9$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weak Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![openmp-weak](1-1-weak_scalability_openmp.png)\n",
    "\n",
    "- 2 cores run has elapsed time:  17.99\n",
    "- 4 cores run has elapsed time:  18.43\n",
    "- 8 cores run has elapsed time:  19.49\n",
    "- 12 cores run has elapsed time:  20.57\n",
    "- 16 cores run has elapsed time:  20.6\n",
    "- 20 cores run has elapsed time:  20.57\n",
    "\n",
    "According to logic of weak scalability when we increase the number of cores with the N run time\n",
    "supposed to be same. However for this example there are small differences. There might be some\n",
    "room for optimization of parallelization part. But after 12 threads run time became more or less constant which is good. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Overhead\n",
    "**2-estimate the parallel overhead;**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
