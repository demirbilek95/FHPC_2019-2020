{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: (warm-up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compute Theoretical Peak performance for your laptop/desktop\n",
    "\n",
    "  - The CPU of my computer is **Intel(R) Core(TM) i7-4720HQ**                       \n",
    "\n",
    "  - Clock-rate (frequency) is 3.6 GHz (Turbo frequency)      \n",
    "\n",
    "  - It has 4 cores \n",
    "  \n",
    "  - Number_of_FP_operation is **16** (for 64 bit architecture) ([source](https://en.wikipedia.org/wiki/FLOPS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theoretical Peak Performance of my computer is: 230.4 GLOPS\n"
     ]
    }
   ],
   "source": [
    "Clock_rate = 3.6 #turbo frequency\n",
    "cores = 4\n",
    "FP_Operation = 16\n",
    "\n",
    "Theoretical_Peak_Performance = Clock_rate * FP_Operation * cores\n",
    "\n",
    "print(\"Theoretical Peak Performance of my computer is:\",Theoretical_Peak_Performance, \"GLOPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|        | Your model | CPU  | Frequency | number  of core | Peak performance |\n",
    "| ------ | ---------- | ---- | --------- | --------------- | ---------------- |\n",
    "| Laptop | MSI GE70 2QE APACHE PRO|Intel(R) Core(TM) i7-4720HQ| 3.6 GHz|4 | 230.4 GLOPS      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compute sustained Peak performance for your  cell-phone\n",
    "    * My phone is iPhone 6, its CPU is **Apple A8 a 64 bit**\n",
    "    \n",
    "    * Clock-rate (frequency) is 1.4 GHz\n",
    "    \n",
    "    * It has 2 cores\n",
    "    \n",
    "    * Number_of_FP_operation is **12.8** (for 64 bit architecture) [source](https://en.wikipedia.org/wiki/Apple-designed_processors#Apple_A4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compute sustained Peak performance for your  cell-phone\n",
    "\n",
    "    * I have used [MobileLinpack](https://apps.apple.com/cz/app/mobile-linpack/id1001893181) app from App Store.\n",
    "    \n",
    "    * Below you can find the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Run\n",
    "\n",
    " **Default Parameters**\n",
    " \n",
    " * Matrix start size = 32\n",
    " * Matrix finish size = 257\n",
    " * Step = 16\n",
    " * Number of iterations = 200\n",
    " \n",
    " * **Result** 2.55 GLOPS\n",
    " \n",
    "### Second Run\n",
    " \n",
    " * Matrix start size = 64\n",
    " * Matrix finish size = 257\n",
    " * Step = 16\n",
    " * Number of iterations = 150\n",
    " \n",
    " * **Result** 2.55 GLOPS\n",
    " \n",
    "### Third Run\n",
    " \n",
    " * Matrix start size = 128\n",
    " * Matrix finish size = 512\n",
    " * Step = 16\n",
    " * Number of iterations = 150\n",
    " \n",
    " * **Result** 2.09 GLOPS\n",
    " \n",
    "### Fourth Run\n",
    "\n",
    " * Matrix start size = 32\n",
    " * Matrix finish size = 512\n",
    " * Step = 8\n",
    " * Number of iterations = 150\n",
    " \n",
    " * **Result** 2.58 GLOPS\n",
    " \n",
    "### Fifth Run\n",
    "\n",
    " * Matrix start size = 16\n",
    " * Matrix finish size = 700\n",
    " * Step = 8\n",
    " * Number of iterations = 200\n",
    " \n",
    " * **Result** 2.58 GLOPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Extra Bonus:  Identify the CPU and calculate the Peak performance of the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theoretical Peak Performance of my cell phone is: 35.839999999999996 GLOPS\n"
     ]
    }
   ],
   "source": [
    "Clock_rate = 1.4\n",
    "cores = 2\n",
    "FP_Operation = 12.8\n",
    "\n",
    "Theoretical_Peak_Performance = Clock_rate * FP_Operation * cores\n",
    "\n",
    "print(\"Theoretical Peak Performance of my cell phone is:\",Theoretical_Peak_Performance, \"GLOPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  |            | Your model | sustained performance | size of matrix | Peak performance | memory |\n",
    "  | ---------- | ---------- | --------------------- | -------------- | ---------------- | ------ |\n",
    "  | SmartPhone | iPhone 6, Apple A8 (CPU)| 2.58 GLOPS |  700 |  35.84 GLOPS | 1 GB   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find out in which year  your cell phone/laptop  could have been in top 1% of Top500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to results my phone is 505.7 times are slower than Intel ASCI Red (1998 TOP500 #1) and 3307.4 times are faster than ES-1045 (1979)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Top500, sustained peak performance is used for benchmark, that is why to answer the below question, I also find the sustained peak performance of my laptop by using **Intel® Math Kernel Library (Intel® MKL) Benchmarks Suite.**\n",
    "\n",
    "Parameters for test is Number of equations to solve (problem size): 30000, Leading dimension of array: 30000, Number of trials to run: 8\n",
    "\n",
    "Average sustained performance is **131.2885 GLOPS**, sustained peak performance is **136.3127 GLOPS**      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even for 1993 my smart phone can not enter the %1 of the TOP500.\n",
    "\n",
    "My laptop can enter the %1 of TOP500 in 1996 (November) as #5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|            | Your model | performance | Top500 year | number 1 HPC system | number of processors (TOP500) |\n",
    "  | ---------- | ---------- | ----------- | ----------- | ------------------- | ----------------------------- |\n",
    "  | SmartPhone |iPhone 6, Apple A8 (CPU)  |2.58 GLOPS   |  Can not enter(1993) |  \tNumerical Wind Tunnel    |             140                  |\n",
    "  | Laptop     | Intel(R) Core(TM) i7-4720HQ  | 136.3 GLOPS   |  1996 (November)   | CP-PACS/2048          |2048                               |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: theoretical model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- devise a performance model for a simple parallel algorithm: sum of N numbers\n",
    "\n",
    "  - Serial Algorithm : n-1 operations \n",
    "\n",
    "     $T_{serial}= N*T_{comp}$  where\n",
    "    $T_{comp}$ *is time to compute a floating point operation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - Parallel Algorithm : master-slave\t\n",
    "\n",
    "    - read N and distribute N to P-1  slaves ===>  $T_{read} + (P-1) * T_{comm}$ where\n",
    "      $T_{comm}$ is *time  each processor takes to communicate one message, i.e. latency..*\n",
    "      $T_{read}$   = *time  master takes to read* \n",
    "\n",
    "    - N/P sum over each processors (including master)  ===> $T_{comp}/N$\n",
    "\n",
    "    - Slaves send partial sum  ===>   $(P-1) * T_{comm}$\n",
    "\n",
    "    - Master performs  one final sum ===>  $(P-1) * T_{comp}$\n",
    "\n",
    "      the final model ===> $T_p=   T_{comp}* (P -1 + n/P)  + T_{read} + 2(P-1) * T_{comm}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* N: numbers to be sum\n",
    "* T_comp: time to compute a floating point operation\n",
    "* T_read: time master takes to read\n",
    "* P: number of processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Assumptions\n",
    "\n",
    "T_comp = 2e-09\n",
    "T_read = 1e-04\n",
    "T_comm = 1e-06\n",
    "P = np.arange(1,1000+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- compute scalability curves for such algorithm and make some plots\n",
    "- Play with some value of N and plot against P  (with P ranging from 1 to 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](logo.png)\n",
    "\n",
    "*Refer to ../codes/section1_scalability.py*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Serial algorithm time for N =  1000000 is  0.002\n",
    "    * For N = 1000000 best speed up value when P is 32 \n",
    "\n",
    "* Serial algorithm time for N =  10000000 is  0.02\n",
    "    * For N = 10000000 best speed up value when P is 100 \n",
    "\n",
    "* Serial algorithm time for N =  100000000 is  0.2\n",
    "    * For N = 100000000 best speed up value when P is 316 \n",
    "\n",
    "* Serial algorithm time for N =  1000000000 is  2.0\n",
    "    * For N = 1000000000 best speed up value when P is 1000 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Section_1 Scalability](section1_scalability.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - For which values of N do you see the algorithm scaling ? \n",
    "\n",
    "  - For which values of P does the algorithm produce the best results ? \n",
    "  \n",
    "For P = [1,1000], when N gets larger, algorithm tends to scale better. \n",
    "\n",
    "Best scaling P values are highly dependent on N, when N is smaller best scaling value of P is small as well. For instances: For N = $10^9$ best scaling value of P is 1000, for N = $10^8$ best scaling value of P is 316, for N $10^7$ best scaling value of P is 100 and for N = $10^6$ best scaling value of P 32. It means that if N is large enough more processors give better results up to specific point. For smaller N, best P tends to be smaller as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](logo.png)\n",
    "\n",
    "*Refer to ../codes/section1_efficiency.py*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Serial algorithm time for N =  1000000 is 0.002\n",
    "    * For N = 1000000 best efficiency value is 0.000890614702267505 with P 32\n",
    "* Serial algorithm time for N =  10000000 is 0.02\n",
    "    * For N = 10000000 best efficiency value is 0.00040144520272982734 with P 100\n",
    "* Serial algorithm time for N =  100000000 is 0.2\n",
    "    * For N = 100000000 best efficiency value is 0.0001466766669209062 with P 316\n",
    "* Serial algorithm time for N =  1000000000 is 2.0\n",
    "    * For N = 1000000000 best efficiency value is 4.878048780487805e-05 with P 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Section_1 Scalability](section1_efficiency.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen from the plot, for the changing values of N, best P values have the most efficient points as same as previous plot. For N = $10^6$ algorithm is most efficient at P = 32, after that point efficiency decrease. For N = $10^9$ , it has more way to best efficient point. However for the range of P = [1,1000], most efficient point is P = 1000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Can you try to modify the algorithm sketched above to increase its scalability ?\n",
    "    * Reducing communication time should increase the scalability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](logo.png)\n",
    "\n",
    "*Refer to ../codes/section1_better_comm.py*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Serial algorithm time for N =  1000000 is  0.002\n",
    "    * For N = 1000000 best speed up value when P is 100 \n",
    "\n",
    "* Serial algorithm time for N =  10000000 is  0.02\n",
    "    * For N = 10000000 best speed up value when P is 315 \n",
    "\n",
    "* Serial algorithm time for N =  100000000 is  0.2\n",
    "    * For N = 100000000 best speed up value when P is 995 \n",
    "\n",
    "* Serial algorithm time for N =  1000000000 is  2.0\n",
    "    * For N = 1000000000 best speed up value when P is 1000 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Section_1 Scalability](section1_reduce_comm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, when I reduce the communication time (for example using collective operations) algorithm tends to scale better. For example, N = $10^6$ best speed up value when P is 100 and for N = $10^7$ best speed up value when P is 315 and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: play with MPI program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1:  compute strong scalability of a mpi_pi.c program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compile the serial and parallel version.\n",
    "    - I compiled pi.c and mpi_pi.c files using ```gcc pi.c -o pi.x``` and ```mpicc mpi_pi.c -o  mpi_pi.x``` commands in Ulysses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Determine the CPU time required to calculate PI with the serial calculation using 10000000 (10 millions) iterations (stone throws). Make sure that this is the actual run time and does not include any system time.\n",
    "    - After that by using ```/usr/bin/time ./pi.x 10000000 ``` and ``` /usr/bin/time mpirun -np 4 ./mpi_pi.x 10000000``` commands I have determined the required time and estimated pi value. You can find the results below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Result of pi.x:\n",
    "\n",
    "of trials = 10000000 , estimate of pi is 3.141396400 \n",
    "\n",
    "walltime : 0.25000000 \n",
    "\n",
    "0.25user 0.00system **0:00.25elapsed** 99%CPU (0avgtext+0avgdata 1936maxresident)k\n",
    "0inputs+0outputs (0major+143minor)pagefaults 0swaps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Elapsed time for serial program is **0.25 seconds**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Result of mpi_pi.x:\n",
    "\n",
    "of trials = 10000000 , estimate of pi is 3.141961600 \n",
    "\n",
    "walltime on master processor : **0.07639313** \n",
    "\n",
    "walltime on processor 1 : 0.06366587 \n",
    "\n",
    "walltime on processor 2 : 0.07195497 \n",
    "\n",
    " walltime on processor 3 : 0.06781292\n",
    " \n",
    "4.83user 0.25system **0:01.70elapsed** 299%CPU (0avgtext+0avgdata 103008maxresident)k\n",
    "0inputs+8outputs (1major+26408minor)pagefaults 0swaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Elapsed time for parallel pi program is **1.70 seconds**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The parallel code writes walltime for all the processor involved. Which of these times do you want to consider to estimate the parallel time ?\n",
    "    * We should consider the **maximum walltime of processors** to estimate the parallel time. In this case it is **0.07639313**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First let us do some running that constitutes a strong scaling test.\n",
    "- Make a plot of run time  versus number of nodes from the data you have collected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "for procs in 1 2 4 8 16 20 ; do /usr/bin/time mpirun -np ${procs} mpi_pi.x 10000000 done > <output.txt files>``` \n",
    "by using above command, I get the output.txt files, after that by using python I get the maximum walltime for each run with different number of processors and with these run times I plotted the strong scalability of algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strong Scaling with Max Walltime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Strong scalability here would yield a straight line graph. Comment on your results. Repeat the work playing with a large dataset.\n",
    "- Provide a final plot with at least 3 different size and for each of the size report and comment your final results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](logo.png)\n",
    "\n",
    "*Refer to ../codes/section2_strong_runTime_maxTime.py*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For N = 10^6  you can find run times for different processors below\n",
    "    * 1 cores run has maximum run time:  0.01990104 scalability is 1.0\n",
    "    * 2 cores run has maximum run time:  0.01076198 scalability is 1.849198753389246\n",
    "    * 4 cores run has maximum run time:  0.00544691 scalability is 3.653638484939167\n",
    "    * 8 cores run has maximum run time:  0.00333095 scalability is 5.974583827436617\n",
    "    * 16 cores run has maximum run time:  0.00339699 scalability is 5.8584334955357535\n",
    "    * 20 cores run has maximum run time:  0.00313091 scalability is 6.3563117432312\n",
    "    \n",
    "\n",
    "* For N = 10^7  you can find run times for different processors below\n",
    "    * 1 cores run has maximum run time:  0.19876003 scalability is 1.0\n",
    "    * 2 cores run has maximum run time:  0.10333514 scalability is 1.9234505319294095\n",
    "    * 4 cores run has maximum run time:  0.051332 scalability is 3.8720492090703655\n",
    "    * 8 cores run has maximum run time:  0.0273509 scalability is 7.267038013374331\n",
    "    * 16 cores run has maximum run time:  0.01808906 scalability is 10.987858407236196\n",
    "    * 20 cores run has maximum run time:  0.01323915 scalability is 15.013050686788805\n",
    "    \n",
    "\n",
    "* For N = 10^8  you can find run times for different processors below\n",
    "    * 1 cores run has maximum run time:  1.98622179 scalability is 1.0\n",
    "    * 2 cores run has maximum run time:  1.0219152 scalability is 1.9436268195247512\n",
    "    * 4 cores run has maximum run time:  0.51305103 scalability is 3.8713922667692535\n",
    "    * 8 cores run has maximum run time:  0.27128315 scalability is 7.321581860133961\n",
    "    * 16 cores run has maximum run time:  0.14891601 scalability is 13.337866022598915\n",
    "    * 20 cores run has maximum run time:  0.11698103 scalability is 16.97900753652109\n",
    "    \n",
    "\n",
    "* For N = 10^9  you can find run times for different processors below\n",
    "    * 1 cores run has maximum run time:  19.85913587 scalability is 1.0\n",
    "    * 2 cores run has maximum run time:  10.22568107 scalability is 1.9420844180504055\n",
    "    * 4 cores run has maximum run time:  5.14612985 scalability is 3.8590429019197794\n",
    "    * 8 cores run has maximum run time:  2.72066188 scalability is 7.299376675943281\n",
    "    * 16 cores run has maximum run time:  1.44949389 scalability is 13.700737896866883\n",
    "    * 20 cores run has maximum run time:  1.16037512 scalability is 17.114410269327387\n",
    "    \n",
    "\n",
    "* For N = 10^10 you can find run times for different processors below\n",
    "    * 1 cores run has maximum run time:  198.1946258 scalability is 1.0\n",
    "    * 2 cores run has maximum run time:  102.394792 scalability is 1.9355928356199994\n",
    "    * 4 cores run has maximum run time:  51.2605071 scalability is 3.8664195306019518\n",
    "    * 8 cores run has maximum run time:  27.986582 scalability is 7.08177317973306\n",
    "    * 16 cores run has maximum run time:  15.0945041 scalability is 13.130250883829964\n",
    "    * 20 cores run has maximum run time:  11.862725 scalability is 16.70734386913631\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section2_strong_scalability_maxTime](section2_strong_runTime_maxTime.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly run time decrease visibly, after specific point for number of nodes it begin to stop.It is compatible with the theoretical model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](logo.png)\n",
    "\n",
    "*Refer to ../codes/section2_strong_scalability_maxTime.py*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For N = 10^6  you can find run times for different processors below\n",
    "    * 1 cores run has maximum run time:  0.01990104 scalability is 1.0\n",
    "    * 2 cores run has maximum run time:  0.01076198 scalability is 1.849198753389246\n",
    "    * 4 cores run has maximum run time:  0.00544691 scalability is 3.653638484939167\n",
    "    * 8 cores run has maximum run time:  0.00333095 scalability is 5.974583827436617\n",
    "    * 16 cores run has maximum run time:  0.00339699 scalability is 5.8584334955357535\n",
    "    * 20 cores run has maximum run time:  0.00313091 scalability is 6.3563117432312\n",
    "    \n",
    "\n",
    "* For N = 10^7  you can find run times for different processors below\n",
    "    * 1 cores run has maximum run time:  0.19876003 scalability is 1.0\n",
    "    * 2 cores run has maximum run time:  0.10333514 scalability is 1.9234505319294095\n",
    "    * 4 cores run has maximum run time:  0.051332 scalability is 3.8720492090703655\n",
    "    * 8 cores run has maximum run time:  0.0273509 scalability is 7.267038013374331\n",
    "    * 16 cores run has maximum run time:  0.01808906 scalability is 10.987858407236196\n",
    "    * 20 cores run has maximum run time:  0.01323915 scalability is 15.013050686788805\n",
    "    \n",
    "\n",
    "* For N = 10^8  you can find run times for different processors below\n",
    "    * 1 cores run has maximum run time:  1.98622179 scalability is 1.0\n",
    "    * 2 cores run has maximum run time:  1.0219152 scalability is 1.9436268195247512\n",
    "    * 4 cores run has maximum run time:  0.51305103 scalability is 3.8713922667692535\n",
    "    * 8 cores run has maximum run time:  0.27128315 scalability is 7.321581860133961\n",
    "    * 16 cores run has maximum run time:  0.14891601 scalability is 13.337866022598915\n",
    "    * 20 cores run has maximum run time:  0.11698103 scalability is 16.97900753652109\n",
    "    \n",
    "\n",
    "* For N = 10^9  you can find run times for different processors below\n",
    "    * 1 cores run has maximum run time:  19.85913587 scalability is 1.0\n",
    "    * 2 cores run has maximum run time:  10.22568107 scalability is 1.9420844180504055\n",
    "    * 4 cores run has maximum run time:  5.14612985 scalability is 3.8590429019197794\n",
    "    * 8 cores run has maximum run time:  2.72066188 scalability is 7.299376675943281\n",
    "    * 16 cores run has maximum run time:  1.44949389 scalability is 13.700737896866883\n",
    "    * 20 cores run has maximum run time:  1.16037512 scalability is 17.114410269327387\n",
    "    \n",
    "\n",
    "* For N = 10^10 you can find run times for different processors below\n",
    "    * 1 cores run has maximum run time:  198.1946258 scalability is 1.0\n",
    "    * 2 cores run has maximum run time:  102.394792 scalability is 1.9355928356199994\n",
    "    * 4 cores run has maximum run time:  51.2605071 scalability is 3.8664195306019518\n",
    "    * 8 cores run has maximum run time:  27.986582 scalability is 7.08177317973306\n",
    "    * 16 cores run has maximum run time:  15.0945041 scalability is 13.130250883829964\n",
    "    * 20 cores run has maximum run time:  11.862725 scalability is 16.70734386913631"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section2_strong_scalability_maxTime](section2_strong_scalability_maxTime.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallel to theoretical model, if we increase the N, algorithm tend to scale good up to specific points (Ahmdals Law). For smaller N even if we increase the number of nodes algorithm doesn't scale good. For N = $10^6$ using more than 8 cores is not logical for other values of N there are still place for improved scalability. But there is one interesting point for N = $10^{10}$, so for this value it doesn't scale better than $10^9$. In my opinion it is because of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strong Scaling with Elapsed Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](logo.png)\n",
    "\n",
    "*Refer to ../codes/section2_strong_runTime_elapsedTime.py*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For N = 10^6  you can find elapsed times for different processors below\n",
    "    * 1 cores run has elapsed time:  1.57\n",
    "    * 2 cores run has elapsed time:  1.56\n",
    "    * 4 cores run has elapsed time:  1.58\n",
    "    * 8 cores run has elapsed time:  1.65\n",
    "    * 16 cores run has elapsed time:  1.78\n",
    "    * 20 cores run has elapsed time:  1.86\n",
    "    \n",
    "\n",
    "* For N = 10^7  you can find elapsed times for different processors below\n",
    "    * 1 cores run has elapsed time:  1.72\n",
    "    * 2 cores run has elapsed time:  1.62\n",
    "    * 4 cores run has elapsed time:  1.59\n",
    "    * 8 cores run has elapsed time:  1.68\n",
    "    * 16 cores run has elapsed time:  1.8\n",
    "    * 20 cores run has elapsed time:  1.89\n",
    "    \n",
    "\n",
    "* For N = 10^8  you can find elapsed times for different processors below\n",
    "    * 1 cores run has elapsed time:  3.47\n",
    "    * 2 cores run has elapsed time:  2.53\n",
    "    * 4 cores run has elapsed time:  2.04\n",
    "    * 8 cores run has elapsed time:  1.85\n",
    "    * 16 cores run has elapsed time:  1.92\n",
    "    * 20 cores run has elapsed time:  1.96\n",
    "    \n",
    "\n",
    "* For N = 10^9  you can find elapsed times for different processors below\n",
    "    * 1 cores run has elapsed time:  21.35\n",
    "    * 2 cores run has elapsed time:  11.8\n",
    "    * 4 cores run has elapsed time:  6.65\n",
    "    * 8 cores run has elapsed time:  4.3\n",
    "    * 16 cores run has elapsed time:  3.19\n",
    "    * 20 cores run has elapsed time:  3.03\n",
    "\n",
    "\n",
    "* For N = 10^10 you can find elapsed times for different processors below\n",
    "    * 1 cores run has elapsed time:  199.69\n",
    "    * 2 cores run has elapsed time:  103.92\n",
    "    * 4 cores run has elapsed time:  52.79\n",
    "    * 8 cores run has elapsed time:  29.57\n",
    "    * 16 cores run has elapsed time:  16.85\n",
    "    * 20 cores run has elapsed time:  13.65\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section2_strong_runtime_elapsedTime](section2_strong_runTime_elapsedTime.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to max wall times, algorithm run time firstly decrease visibly, after some specific point this decrease begin to stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](logo.png)\n",
    "\n",
    "*Refer to ../codes/section2_strong_scalability_elapsedTime.py*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section2_strong_scalability_elapsedTime](section2_strong_scalability_elapsed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar results are found for the same analyisis but this time with elapsed time. As previously mentioned algorithm tends to scale better when N is increased. However after some specific points for P, algorithm doesn't scale even P is increased. For example, it is not logical to increase P after 8 for N = $10^8$. Because elapsed time increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: identify a model for the parallel overhead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks in a program can be split into serial tasks executed by a single worker and parallel tasks\n",
    "distributed to multiple workers. Then, the total runtime (execution time) of a program with n\n",
    "workers is given as:\n",
    "\n",
    "$T(n) = T_p/n + T_s+ P(n)$\n",
    "\n",
    "where $T_p$ is the runtime of parallelizable tasks with a single core, $T_s$ is the runtime of a serial\n",
    "code, and $P(n$) is parallelization overhead. When a set of tasks is distributed to n workers, then\n",
    "the ideal computing time of the parallelized tasks should be $T_p/n$. However, the actual time to\n",
    "compute the tasks in parallel requires extra P(n) seconds. Therefore, the parallelization overhead\n",
    "P(n) is defined as the additional time incurred from parallelizing a chunk of tasks to multiple\n",
    "processors. [source](https://editorialexpress.com/cgi-bin/conference/download.cgi?db_name=CEF2019&paper_id=358)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I couldn't determine the serial part in mpi_pi.c file and since it is constant it is not crucial to understand the overhead change with increasing number of P so I considered $T_S$ as negligible, so P(n) is $P(n) = (T(n) - T_s)-T_p/n$.Final equaition for parallel overhead is $P(n) = T(n)-T_p/n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](logo.png)\n",
    "\n",
    "*Refer to ../codes/section2_overhead_maxTime.py*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Wall Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For N = 10^6  you can find run times and overheads for different processors below\n",
    "    * 1 cores run has maximum run time:  0.01990104 overhead is 0.0\n",
    "    * 2 cores run has maximum run time:  0.01076198 overhead is 0.00538099\n",
    "    * 4 cores run has maximum run time:  0.00544691 overhead is 0.00413537\n",
    "    * 8 cores run has maximum run time:  0.00333095 overhead is 0.0029799675\n",
    "    * 16 cores run has maximum run time:  0.00339699 overhead is 0.0032974356249999997\n",
    "    * 20 cores run has maximum run time:  0.00313091 overhead is 0.0030608625000000003\n",
    "    \n",
    "\n",
    "* For N = 10^7  you can find run times and overheads for different processors below\n",
    "    * 1 cores run has maximum run time:  0.19876003 overhead is 0.0\n",
    "    * 2 cores run has maximum run time:  0.10333514 overhead is 0.05166757\n",
    "    * 4 cores run has maximum run time:  0.051332 overhead is 0.03850049\n",
    "    * 8 cores run has maximum run time:  0.0273509 overhead is 0.0239531375\n",
    "    * 16 cores run has maximum run time:  0.01808906 overhead is 0.017179314375\n",
    "    * 20 cores run has maximum run time:  0.01323915 overhead is 0.01265295\n",
    "\n",
    "* For N = 10^8  you can find run times and overheads for different processors below\n",
    "    * 1 cores run has maximum run time:  1.98622179 overhead is 0.0\n",
    "    * 2 cores run has maximum run time:  1.0219152 overhead is 0.5109576\n",
    "    * 4 cores run has maximum run time:  0.51305103 overhead is 0.3848860224999999\n",
    "    * 8 cores run has maximum run time:  0.27128315 overhead is 0.23737275624999998\n",
    "    * 16 cores run has maximum run time:  0.14891601 overhead is 0.13988595124999997\n",
    "    * 20 cores run has maximum run time:  0.11698103 overhead is 0.111198986\n",
    "\n",
    "* For N = 10^9  you can find run times and overheads for different processors below\n",
    "    * 1 cores run has maximum run time:  19.85913587 overhead is 0.0\n",
    "    * 2 cores run has maximum run time:  10.22568107 overhead is 5.112840535\n",
    "    * 4 cores run has maximum run time:  5.14612985 overhead is 3.8623858725000004\n",
    "    * 8 cores run has maximum run time:  2.72066188 overhead is 2.3805810225000004\n",
    "    * 16 cores run has maximum run time:  1.44949389 overhead is 1.359477256875\n",
    "    * 20 cores run has maximum run time:  1.16037512 overhead is 1.1026759640000001\n",
    "\n",
    "* For N = 10^10 you can find run times and overheads for different processors below\n",
    "    * 1 cores run has maximum run time:  198.1946258 overhead is 0.0\n",
    "    * 2 cores run has maximum run time:  102.394792 overhead is 51.197396\n",
    "    * 4 cores run has maximum run time:  51.2605071 overhead is 38.445380325\n",
    "    * 8 cores run has maximum run time:  27.986582 overhead is 24.586386649999998\n",
    "    * 16 cores run has maximum run time:  15.0945041 overhead is 14.1934852875\n",
    "    * 20 cores run has maximum run time:  11.862725 overhead is 11.286937649999999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we increase the number of cores overhead tends to be smaller. Overhead must be zero for 1 cores because for 1 cores $T_n = T_s, and ,n = 1$ it makes $P(n) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](logo.png)\n",
    "\n",
    "*Refer to ../codes/section2_overhead_elapsedTime.py*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elapsed time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For N = 10^6  you can find elapsed times for different processors below\n",
    "    * 1 cores run has elpased time:  1.5\n",
    "    * 2 cores run has elpased time:  1.56\n",
    "    * 4 cores run has elpased time:  1.58\n",
    "    * 8 cores run has elpased time:  1.65\n",
    "    * 16 cores run has elpased time:  1.78\n",
    "    * 20 cores run has elpased time:  1.86\n",
    "\n",
    "**Overhead for N 10^6  [0.0, 0.775, 1.1875, 1.4537499999999999, 1.681875, 1.7815]**\n",
    "\n",
    "* For N = 10^7  you can find elapsed times for different processors below\n",
    "    * 1 cores run has elpased time:  1.72\n",
    "    * 2 cores run has elpased time:  1.62\n",
    "    * 4 cores run has elpased time:  1.59\n",
    "    * 8 cores run has elpased time:  1.68\n",
    "    * 16 cores run has elpased time:  1.8\n",
    "    * 20 cores run has elpased time:  1.89\n",
    "\n",
    "**Overhead for N 10^7  [0.0, 0.7600000000000001, 1.1600000000000001, 1.4649999999999999, 1.6925000000000001, 1.8039999999999998]**\n",
    "\n",
    "* For N = 10^8  you can find elapsed times for different processors below\n",
    "    * 1 cores run has elpased time:  3.47\n",
    "    * 2 cores run has elpased time:  2.53\n",
    "    * 4 cores run has elpased time:  2.04\n",
    "    * 8 cores run has elpased time:  1.85\n",
    "    * 16 cores run has elpased time:  1.92\n",
    "    * 20 cores run has elpased time:  1.96\n",
    "\n",
    "**Overhead for N 10^8  [0.0, 0.7949999999999997, 1.1724999999999999, 1.41625, 1.703125, 1.7865**\n",
    "\n",
    "* For N = 10^9  you can find elapsed times for different processors below\n",
    "    * 1 cores run has elpased time:  21.35\n",
    "    * 2 cores run has elpased time:  11.8\n",
    "    * 4 cores run has elpased time:  6.65\n",
    "    * 8 cores run has elpased time:  4.3\n",
    "    * 16 cores run has elpased time:  3.19\n",
    "    * 20 cores run has elpased time:  3.03\n",
    "\n",
    "**Overhead for N 10^9  [0.0, 1.125, 1.3125, 1.6312499999999996, 1.8556249999999999, 1.9624999999999997]**\n",
    "\n",
    "* For N = 10^10 you can find elapsed times for different processors below\n",
    "    * 1 cores run has elapsed time:  199.69\n",
    "    * 2 cores run has elapsed time:  103.92\n",
    "    * 4 cores run has elpased time:  52.79\n",
    "    * 8 cores run has elpased time:  29.57\n",
    "    * 16 cores run has elpased time:  16.85\n",
    "    * 20 cores run has elpased time:  13.65\n",
    "\n",
    "**Overhead for N 10^10 [0.0, 4.075000000000003, 2.8674999999999997, 4.608750000000001, 4.369375000000002, 3.6654999999999998]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3: weak scaling  with max walltime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now let us do some running that constitutes a weak scaling test.\n",
    "    - This means increasing the problem size simultaneously with the number of nodes being used. In the present case, increasing the number of iterations, Niter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have used this bash script to increase the problem size simultaneously with the number of nodes being used.\n",
    "\n",
    "```bash\n",
    "for procs in 1 2 4 8 16 20 ; do /usr/bin/time mpirun -np ${procs} ./mpi_pi.x $((${procs}*10000000)); done &> output_weakscaling.txt```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Record the run time for each number of nodes and make a plot of the run time versus number of computing nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](logo.png)\n",
    "\n",
    "*Refer to ../codes/section2_weakscaling_maxTime.py*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1 cores run has maximum run time:  0.198735\n",
    "* 2 cores run has maximum run time:  0.20475984\n",
    "* 4 cores run has maximum run time:  0.20504093\n",
    "* 8 cores run has maximum run time:  0.21748805\n",
    "* 16 cores run has maximum run time:  0.23106313\n",
    "* 20 cores run has maximum run time:  0.2338779\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section2_weakscaling_maxTime](section2_weakscaling_maxTime.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to logic of weak scalability when we increase the number of cores with the N run time supposed to be same. However for this example there are small differences. There might be some room for optimizaiton of parallelization part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot on the same graph the efficiency (T(1)/T(p)) of weak scalability for different number of moves and comment the results obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](logo.png)\n",
    "\n",
    "*Refer to ../codes/section2_weakscaling_maxTime_2.py*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section2_weakscaling_efficiency_maxTime](section2_weakscaling_efficiency_maxTime.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar results as previous findings, for the efficiency of weak scalability decrease when N is increased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weak scaling with elapsed time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](logo.png)\n",
    "\n",
    "*Refer to ../codes/section2_weakscaling_elapsedTime.py*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1 cores run has elapsed time:  1.69\n",
    "* 2 cores run has elapsed time:  1.71\n",
    "* 4 cores run has elapsed time:  1.71\n",
    "* 8 cores run has elapsed time:  1.82\n",
    "* 16 cores run has elapsed time:  1.99\n",
    "* 20 cores run has elapsed time:  1.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section2_weakscaling_elapsedTime](section2_weakscaling_elapsedTime.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](logo.png)\n",
    "\n",
    "*Refer to ../codes/section2_weakscaling_elapsedTime.py*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section2_weakscaling_efficiency_elapsedTime](section2_weakscaling_efficiency_elapsedTime.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar results are obtained from the analysis of weak scalability for elapsed time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: implement a parallel program using MPI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Communicaiton\n",
    "\n",
    "For section 3, I have used python to implement the algorithm. Program read an input from file input_file.txt. Every processors sum its partial array and send it to master processor, master processor collect the all partial results and if there is reminder deal with it. Finally master processor adds all the parital sums and reminder sum if there is.\n",
    "\n",
    "![logo](logo.png)\n",
    "\n",
    "*Refer to ../codes/sumNumbers_mpi.py*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First performance of program was not good. Therefore I tried to make it better. For example I was creating whole array for all processors from 1 to N. I modified and eliminate this part. Run time decreased after this change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collective Communicaiton\n",
    "\n",
    "Most of the logic is same for this approach, only difference is communication channel which is reduce for this part.\n",
    "\n",
    "![logo](logo.png)\n",
    "\n",
    "*Refer to ../codes/sumNumbers_mpi_collective.py*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* measure the reading time, the communication times (initial distribution phase and final collectin phase) and the computation time (hints: use MPI_walltime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I modified sumNumbers.py file to determine reading time, communicaiton times and computation time. I run my program for 4 cores in Ulysses and save the results section4_output_assumptions.txt. After that I sum all times to find final results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4 (run and compare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run the implemented program for different large enough sizes of the array\n",
    "    * I run the program for different large enough sizes of the array by using ```\n",
    "for arrays in 1000 10000 99999 1000000 9999999 10000000 ; do mpiexec -n 4 python sumNumbers_mpi.py ${arrays} ; done > section4_output_different_size_arrays.txt\n",
    "```. \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot as in section 3 scalability of the program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Time Naive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](logo.png)\n",
    "\n",
    "*Refer to ../codes/section4_runtime&scalability_naive_maxTime.py*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1 cores has maximum run time:  0.2748489379882812\n",
    "* 2 cores has maximum run time:  0.1589272022247314\n",
    "* 4 cores has maximum run time:  0.0910718441009521\n",
    "* 8 cores has maximum run time:  0.066909074783325\n",
    "* 16 cores has maximum run time:  0.0644559860229492\n",
    "* 20 cores has maximum run time:  0.0397398471832275"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section4_runtime_naive](section4_runtime_naive.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section4_strong_scalability_naive](section4_strong_scalability_naive.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to results when I increase the number of nodes run time decrease and it scales. But there are some problem about unbalance (especially when there is reminder run time of master processors take more time e.g node = 16) of work between the processors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elapsed Time Naive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](logo.png)\n",
    "\n",
    "*Refer to ../codes/section4_runtime&scalability_naive_elapsed.py*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1 cores run has elapsed time:  0.66\n",
    "* 2 cores run has elapsed time:  0.56\n",
    "* 4 cores run has elapsed time:  0.54\n",
    "* 8 cores run has elapsed time:  0.6\n",
    "* 16 cores run has elapsed time:  0.81\n",
    "* 20 cores run has elapsed time:  0.88"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section4_strong_scalability_naive](section4_runtime_naive_elapsed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Time Collective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](logo.png)\n",
    "\n",
    "*Refer to ../codes/section4_runtime\\&scalability_collective_maxTime.py*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1 cores has maximum run time:  0.0600860118865966\n",
    "* 2 cores has maximum run time:  0.0402710437774658\n",
    "* 4 cores has maximum run time:  0.040165901184082\n",
    "* 8 cores has maximum run time:  0.03432393074035644\n",
    "* 16 cores has maximum run time:  0.0325651168823242\n",
    "* 20 cores has maximum run time:  0.0436670780181884\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section4_runtime\\&scalability_collective_maxTime](section4_strong_runTime_collective.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section4_strong_scalability_collective.png](section4_strong_scalability_collective.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My program for collective operation has some problem. Maybe run time dominating from communication time when I increase the node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elapsed Time Collective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](logo.png)\n",
    "\n",
    "*Refer to ../codes/section4_runtime&scalability_collective_elapsed.py*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1 cores run has elapsed time:  0.65\n",
    "* 2 cores run has elapsed time:  0.53\n",
    "* 4 cores run has elapsed time:  0.49\n",
    "* 8 cores run has elapsed time:  0.58\n",
    "* 16 cores run has elapsed time:  0.78\n",
    "* 20 cores run has elapsed time:  0.94"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![section4_strong_scalability_collective.png](section4_runtime_collective_elapsed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* compare performance results obtained against the performance model elaborated in section 2\n",
    "\n",
    "Program in the section 2 was more stable then my program and it scales better then my program. Apperantly the correction that I have done is not sufficient make it more stable and scalable. Extra steps should be applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* comment on the assumption made in section 1 about times: are they correct ? do you observe something different ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assumptions\n",
    "T_comp = 2e-09\n",
    "T_read = 1e-04\n",
    "T_comm = 1e-06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I measure reading time for master processor by modifying sumNumbers_mpi.py file for 4 cores and reading time is $0.00017404556274414062$ which is more or less same with the assumption(for master node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computaiton time for master is $0.03702187538146973$ and slaves ($0.03630995750427246$, $0.029160022735595703$, $0.027962923049926758$) totally $0.10129475593566895$ which is different from theoretical model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly I measure the communication time (sending and receiving seperately) slave 1 to master node ending is $0.00012493133544921875$ , slave 2 to master node sending $0.029160022735595703$, slave 3 to master node sending time is $0.027962923049926758$ which is totaly $0.05724787712097168$ sending time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Receving time respectively $1.4066696166992188e-05$, $7.891654968261719e-05$ and   which is totaly $0.03824305534362793$. Total communicaiton time is $0.09549093246459961$. It is also different from assumptions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
